{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473de549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df946a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = pd.read_csv('kaggle_poem_dataset.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f75417",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False # Change to true if model wants to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80478e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take just the poems\n",
    "poems = data[\"Content\"]\n",
    "\n",
    "# Remove duplicates\n",
    "poems.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049ee1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate poems to one string\n",
    "concatPoems = ''\n",
    "\n",
    "# How many poems to select\n",
    "poemCount = 50 \n",
    "\n",
    "# Take n poems\n",
    "nPoems = poems[3431:3481]\n",
    "\n",
    "for content in nPoems:\n",
    "    str = content.replace('\\xa0','') # Remove double space\n",
    "    concatPoems += str+ '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b78abca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus: 3526\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the poems\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Define a corpus\n",
    "corpus = concatPoems.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print('Total number of words in corpus:',total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e73f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input sequences using list of tokens\n",
    "sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Max sequence length\n",
    "max_sequence_len = 0\n",
    "for x in sequences:\n",
    "    current_len = len(x)\n",
    "    if(max_sequence_len < current_len):\n",
    "        max_sequence_len = current_len\n",
    "\n",
    "# Pad the sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "837d714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0 1133   80]\n",
      " [   0    0    0 ... 1133   80  292]\n",
      " [   0    0    0 ...   80  292    1]\n",
      " ...\n",
      " [   0    0    0 ...  830    3    1]\n",
      " [   0    0    0 ...    3    1  115]\n",
      " [   0    0    0 ...    1  115  101]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9ce86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0 1133]\n",
      " [   0    0    0 ...    0 1133   80]\n",
      " [   0    0    0 ... 1133   80  292]\n",
      " ...\n",
      " [   0    0    0 ...    4  830    3]\n",
      " [   0    0    0 ...  830    3    1]\n",
      " [   0    0    0 ...    3    1  115]]\n"
     ]
    }
   ],
   "source": [
    "# create X and y\n",
    "X = sequences[:, :-1]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c17823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 80 292   1 ...   1 115 101]\n"
     ]
    }
   ],
   "source": [
    "y = sequences[:,-1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03350397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for y\n",
    "y = to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecad86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim=total_words, output_dim=100, input_length=max_sequence_len-1))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(200,return_sequences=True)))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(total_words,activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b8cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f80158dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    model = create_best_model()\n",
    "\n",
    "    #Tensorboard\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True, update_freq='epoch')\n",
    "\n",
    "     #Checkpoint\n",
    "    chekpoint_path= \"weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(chekpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    callbacks = [tensorboard, checkpoint]\n",
    "\n",
    "    history = model.fit(X, y, batch_size=32, epochs=150, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d28def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8485321402549744\n",
      "Loss: 0.5662724375724792\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    accuracy = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    print(\"Accuracy:\", max(accuracy))\n",
    "    print(\"Loss:\", min(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74b66467",
   "metadata": {},
   "outputs": [],
   "source": [
    "chekpoint_path= \"weights.best.hdf5\"\n",
    "model = load_model(chekpoint_path)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72c10883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_text = \"Help me\"\n",
    "next_words = 22\n",
    "\n",
    "for num in range(next_words):\n",
    "    token = tokenizer.texts_to_sequences([test_text])\n",
    "    new_pad = pad_sequences(token, maxlen=173)\n",
    "    predicted = model.predict(new_pad, verbose=0)\n",
    "\n",
    "    classes_x=np.argmax(predicted,axis=1)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == classes_x:\n",
    "            output_word = word\n",
    "            break\n",
    "    test_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f41a98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me come up with a strategy to get through this nasty pond the big lounge room not night a full moon covers it\n"
     ]
    }
   ],
   "source": [
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ded05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_text = \"Water\"\n",
    "next_words = 34\n",
    "\n",
    "for num in range(next_words):\n",
    "    token = tokenizer.texts_to_sequences([test_text])\n",
    "    new_pad = pad_sequences(token, maxlen=173)\n",
    "    predicted = model.predict(new_pad, verbose=0)\n",
    "\n",
    "    classes_x=np.argmax(predicted,axis=1)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == classes_x:\n",
    "            output_word = word\n",
    "            break\n",
    "    test_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7175f4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water surrounds all shapes that enter up with the frozen seed from dazzled snows your daughter man taken to the last of the sea â€” voice away other than long long wrong shield in dust\n"
     ]
    }
   ],
   "source": [
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "498ec4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_text = \"The snowy mountains painted the sky\"\n",
    "next_words = 25\n",
    "\n",
    "for num in range(next_words):\n",
    "    token = tokenizer.texts_to_sequences([test_text])\n",
    "    new_pad = pad_sequences(token, maxlen=173)\n",
    "    predicted = model.predict(new_pad, verbose=0)\n",
    "\n",
    "    classes_x=np.argmax(predicted,axis=1)\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == classes_x:\n",
    "            output_word = word\n",
    "            break\n",
    "    test_text += \" \" + output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e6bc311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The snowy mountains painted the sky lacy with lightning and the beyond sailboats taken away away out of belief eating the masters of bluff not a toy the drone could have\n"
     ]
    }
   ],
   "source": [
    "print(test_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab2764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
